{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b16f877d-0d29-4147-9972-dd70462356d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b439a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'data/ATE_Test.json', 'r') as f:\n",
    "    ATE_test_data = json.load(f)\n",
    "with open(r'data/NER_Test.json', 'r') as f:\n",
    "    NER_test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739b1baf",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bff4090a-d0ce-4fd1-a8ea-e69d512c93b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4765/2692264229.py:23: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845868/work/torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  input_ids = torch.tensor(padded_embeddings)\n"
     ]
    }
   ],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "def convert_data_to_tensors_word2vec(data, word_vectors):\n",
    "    texts = [data[key]['text'] for key in data]\n",
    "\n",
    "    # Convert texts to word embeddings\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        embedding = []\n",
    "        for word in text.split():\n",
    "            if word in word_vectors:\n",
    "                embedding.append(word_vectors[word])\n",
    "            else:\n",
    "                embedding.append([0] * len(word_vectors['hello']))  # Use a zero vector for unknown words\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    # Pad sequences to have the same length\n",
    "    max_len = 83 # maximum length of word in 1 text \n",
    "    padded_embeddings = []\n",
    "    for embedding in embeddings:\n",
    "        padded_embedding = embedding + [[0] * len(word_vectors['hello'])] * (max_len - len(embedding))\n",
    "        padded_embeddings.append(padded_embedding)\n",
    "\n",
    "    input_ids = torch.tensor(padded_embeddings)\n",
    "\n",
    "    return input_ids\n",
    "x_test_ATE_word2vec = convert_data_to_tensors_word2vec(ATE_test_data, word_vectors)\n",
    "x_test_NER_word2vec = convert_data_to_tensors_word2vec(NER_test_data, word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bae2f0c",
   "metadata": {},
   "source": [
    "## Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8bcc6ae-0f9c-4195-a9fd-8e846278ab6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "400000 words loaded!\n"
     ]
    }
   ],
   "source": [
    "def load_glove_model(File):\n",
    "    print(\"Loading Glove Model\")\n",
    "    glove_model = {}\n",
    "    with open(File,'r') as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "            glove_model[word] = embedding\n",
    "    print(f\"{len(glove_model)} words loaded!\")\n",
    "    return glove_model\n",
    "\n",
    "# Path to your GloVe pre-trained embeddings file\n",
    "glove_file_path = 'glove.6B.300d.txt'\n",
    "\n",
    "# Load GloVe embeddings\n",
    "model_glove = load_glove_model(glove_file_path)\n",
    "\n",
    "def convert_data_to_tensors_glove(data, model):\n",
    "    texts = [data[key]['text'] for key in data]\n",
    "\n",
    "    # Convert texts to word embeddings\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        embedding = []\n",
    "        for word in text.split():\n",
    "            # Check if word exists in the model's vocabulary\n",
    "            if word in model:\n",
    "                embedding.append(model[word])\n",
    "            else:\n",
    "                # If word not found, use zero vector\n",
    "                embedding.append([0] * model[\"hello\"].size)\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    # Pad sequences to have the same length\n",
    "    max_len = 83  # maximum length of word in a text\n",
    "    padded_embeddings = []\n",
    "    for embedding in embeddings:\n",
    "        padded_embedding = embedding + [[0] * model[\"hello\"].size] * (max_len - len(embedding))\n",
    "        padded_embeddings.append(padded_embedding)\n",
    "\n",
    "    # Filter out None values\n",
    "    padded_embeddings = [embedding for embedding in padded_embeddings if embedding is not None]\n",
    "\n",
    "    # Convert to tensor\n",
    "    input_ids = torch.tensor(padded_embeddings, dtype=torch.float32)\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "x_test_ATE_glove = convert_data_to_tensors_glove(ATE_test_data, model_glove)\n",
    "x_test_NER_glove = convert_data_to_tensors_glove(NER_test_data, model_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba54d12",
   "metadata": {},
   "source": [
    "## Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77a94a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "model_fasttext = fasttext.load_model(\"cc.en.300.bin\")\n",
    "def convert_data_to_tensors_fasttext(data, model):\n",
    "    texts = [data[key]['text'] for key in data]\n",
    "\n",
    "    # Convert texts to word embeddings\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        embedding = []\n",
    "        for word in text.split():\n",
    "                embedding.append(model.get_word_vector(word))\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    # Pad sequences to have the same length\n",
    "    max_len = 83 # maximum length of word in 1 text \n",
    "    padded_embeddings = []\n",
    "    for embedding in embeddings:\n",
    "        padded_embedding = embedding + [[0] * len(model.get_word_vector(\"Hello\"))] * (max_len - len(embedding))\n",
    "        padded_embeddings.append(padded_embedding)\n",
    "\n",
    "    input_ids = torch.tensor(padded_embeddings)\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "x_test_ATE_fasttext = convert_data_to_tensors_fasttext(ATE_test_data, model_fasttext)\n",
    "x_test_NER_fasttext = convert_data_to_tensors_fasttext(NER_test_data, model_fasttext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a1a1be",
   "metadata": {},
   "source": [
    "## label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82118b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_fixed_length_ATE(labels, max_length):\n",
    "    new_list=[]\n",
    "    label_to_index_t2 = {'B': 0, 'I': 1, 'O': 2,'<pad>':3}\n",
    "    fixed_length_labels = np.zeros((len(labels), max_length))\n",
    "    for i, example_labels in enumerate(labels):\n",
    "        for j, label in enumerate(example_labels[:max_length]):\n",
    "            fixed_length_labels[i, j] = label_to_index_t2[label]\n",
    "        for k in range(len(example_labels[:max_length]),max_length):\n",
    "            fixed_length_labels[i, k] = 3\n",
    "        new_list.append(len(example_labels[:max_length]))\n",
    "    return fixed_length_labels,new_list\n",
    "\n",
    "def convert_labels_to_fixed_length_NER(labels, max_length):\n",
    "    new_list=[]\n",
    "    label_to_index_t1 = {'I_WITNESS': 0, 'B_JUDGE': 1, 'I_CASE_NUMBER': 2, 'B_CASE_NUMBER': 3, 'I_PROVISION': 4, 'B_STATUTE': 5, 'I_DATE': 6, 'I_STATUTE': 7, 'B_WITNESS': 8, 'B_DATE': 9, 'I_RESPONDENT': 10, 'B_PRECEDENT': 11, 'B_GPE': 12, 'I_ORG': 13, 'I_PETITIONER': 14, 'B_PROVISION': 15, 'B_ORG': 16, 'I_JUDGE': 17, 'I_OTHER_PERSON': 18, 'B_COURT': 19, 'B_PETITIONER': 20, 'B_RESPONDENT': 21, 'I_PRECEDENT': 22, 'I_COURT': 23, 'I_GPE': 24, 'B_OTHER_PERSON': 25, 'O': 26, '<pad>':27}\n",
    "    fixed_length_labels = np.zeros((len(labels), max_length))\n",
    "    for i, example_labels in enumerate(labels):\n",
    "        for j, label in enumerate(example_labels[:max_length]):\n",
    "            fixed_length_labels[i, j] = label_to_index_t1[label]\n",
    "        for k in range(len(example_labels[:max_length]),max_length):\n",
    "            fixed_length_labels[i, k] = 27\n",
    "        new_list.append(len(example_labels[:max_length]))\n",
    "    return fixed_length_labels,new_list\n",
    "\n",
    "max_length_ATE = 83\n",
    "test_labels_ATE = [ATE_test_data[key]['labels'] for key in ATE_test_data]\n",
    "test_lab_ATE,length_test_ATE = convert_labels_to_fixed_length_ATE(test_labels_ATE, max_length_ATE)\n",
    "y_test_ATE = torch.tensor(test_lab_ATE)\n",
    "\n",
    "max_length_NER = 70\n",
    "test_labels_NER = [NER_test_data[key]['labels'] for key in NER_test_data]\n",
    "test_lab_NER,length_test_NER = convert_labels_to_fixed_length_NER(test_labels_NER, max_length_NER)\n",
    "y_test_NER = torch.tensor(test_lab_NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b597ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b69977c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4765/4135077071.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_ATE_word2vec_tensor = torch.tensor(x_test_ATE_word2vec, dtype=torch.float32)\n",
      "/tmp/ipykernel_4765/4135077071.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_ATE_glove_tensor = torch.tensor(x_test_ATE_glove, dtype=torch.float32)\n",
      "/tmp/ipykernel_4765/4135077071.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_ATE_fasttext_tensor = torch.tensor(x_test_ATE_fasttext, dtype=torch.float32)\n",
      "/tmp/ipykernel_4765/4135077071.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_NER_word2vec_tensor = torch.tensor(x_test_NER_word2vec, dtype=torch.float32)\n",
      "/tmp/ipykernel_4765/4135077071.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_NER_glove_tensor = torch.tensor(x_test_NER_glove, dtype=torch.float32)\n",
      "/tmp/ipykernel_4765/4135077071.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_NER_fasttext_tensor = torch.tensor(x_test_NER_fasttext, dtype=torch.float32)\n",
      "/tmp/ipykernel_4765/4135077071.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_ATE_tensor = torch.tensor(y_test_ATE, dtype=torch.long)\n",
      "/tmp/ipykernel_4765/4135077071.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_NER_tensor = torch.tensor(y_test_NER, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "x_test_ATE_word2vec_tensor = torch.tensor(x_test_ATE_word2vec, dtype=torch.float32)\n",
    "x_test_ATE_glove_tensor = torch.tensor(x_test_ATE_glove, dtype=torch.float32)\n",
    "x_test_ATE_fasttext_tensor = torch.tensor(x_test_ATE_fasttext, dtype=torch.float32)\n",
    "\n",
    "x_test_NER_word2vec_tensor = torch.tensor(x_test_NER_word2vec, dtype=torch.float32)\n",
    "x_test_NER_glove_tensor = torch.tensor(x_test_NER_glove, dtype=torch.float32)\n",
    "x_test_NER_fasttext_tensor = torch.tensor(x_test_NER_fasttext, dtype=torch.float32)\n",
    "\n",
    "y_test_ATE_tensor = torch.tensor(y_test_ATE, dtype=torch.long)\n",
    "y_test_NER_tensor = torch.tensor(y_test_NER, dtype=torch.long)\n",
    "\n",
    "length_test_ATE_tensor = torch.tensor(length_test_ATE)\n",
    "length_test_NER_tensor = torch.tensor(length_test_NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa651052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "class RNNTagger(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNTagger, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.rnn.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "#define the LSTM model\n",
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(1, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0,c0))\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "#define the GRU model\n",
    "class GRUTagger(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GRUTagger, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.gru.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 300 \n",
    "hidden_size = 128\n",
    "output_size_ATE = 4\n",
    "output_size_NER = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ff687c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_t1_model1_word2vec = RNNTagger(input_size, hidden_size, output_size_NER)\n",
    "model_t1_model1_word2vec.load_state_dict(torch.load(\"t1_model1_word2vec.pth\"))\n",
    "model_t2_model1_word2vec = RNNTagger(input_size, hidden_size, output_size_ATE)\n",
    "model_t2_model1_word2vec.load_state_dict(torch.load(\"t2_model1_word2vec.pth\"))\n",
    "model_t1_model1_glove = RNNTagger(input_size, hidden_size, output_size_NER)\n",
    "model_t1_model1_glove.load_state_dict(torch.load(\"t1_model1_glove.pth\"))\n",
    "model_t2_model1_glove = RNNTagger(input_size, hidden_size, output_size_ATE)\n",
    "model_t2_model1_glove.load_state_dict(torch.load(\"t2_model1_glove.pth\"))\n",
    "model_t1_model1_fasttext = RNNTagger(input_size, hidden_size, output_size_NER)\n",
    "model_t1_model1_fasttext.load_state_dict(torch.load(\"t1_model1_fasttext.pth\"))\n",
    "model_t2_model1_fasttext = RNNTagger(input_size, hidden_size, output_size_ATE)\n",
    "model_t2_model1_fasttext.load_state_dict(torch.load(\"t2_model1_fasttext.pth\"))\n",
    "\n",
    "model_t1_model2_word2vec = LSTMTagger(input_size, hidden_size, output_size_NER)\n",
    "model_t1_model2_word2vec.load_state_dict(torch.load(\"t1_model2_word2vec.pth\"))\n",
    "model_t2_model2_word2vec = LSTMTagger(input_size, hidden_size, output_size_ATE)\n",
    "model_t2_model2_word2vec.load_state_dict(torch.load(\"t2_model2_word2vec.pth\"))\n",
    "model_t1_model2_glove = LSTMTagger(input_size, hidden_size, output_size_NER)\n",
    "model_t1_model2_glove.load_state_dict(torch.load(\"t1_model2_glove.pth\"))\n",
    "model_t2_model2_glove = LSTMTagger(input_size, hidden_size, output_size_ATE)\n",
    "model_t2_model2_glove.load_state_dict(torch.load(\"t2_model2_glove.pth\"))\n",
    "model_t1_model2_fasttext = LSTMTagger(input_size, hidden_size, output_size_NER)\n",
    "model_t1_model2_fasttext.load_state_dict(torch.load(\"t1_model2_fasttext.pth\"))\n",
    "model_t2_model2_fasttext = LSTMTagger(input_size, hidden_size, output_size_ATE)\n",
    "model_t2_model2_fasttext.load_state_dict(torch.load(\"t2_model2_fasttext.pth\"))\n",
    "\n",
    "model_t1_model3_word2vec = GRUTagger(input_size, hidden_size, output_size_NER)\n",
    "model_t1_model3_word2vec.load_state_dict(torch.load(\"t1_model3_word2vec.pth\"))\n",
    "model_t2_model3_word2vec = GRUTagger(input_size, hidden_size, output_size_ATE)\n",
    "model_t2_model3_word2vec.load_state_dict(torch.load(\"t2_model3_word2vec.pth\"))\n",
    "model_t1_model3_glove = GRUTagger(input_size, hidden_size, output_size_NER)\n",
    "model_t1_model3_glove.load_state_dict(torch.load(\"t1_model3_glove.pth\"))\n",
    "model_t2_model3_glove = GRUTagger(input_size, hidden_size, output_size_ATE)\n",
    "model_t2_model3_glove.load_state_dict(torch.load(\"t2_model3_glove.pth\"))\n",
    "model_t1_model3_fasttext = GRUTagger(input_size, hidden_size, output_size_NER)\n",
    "model_t1_model3_fasttext.load_state_dict(torch.load(\"t1_model3_fasttext.pth\"))\n",
    "model_t2_model3_fasttext = GRUTagger(input_size, hidden_size, output_size_ATE)\n",
    "model_t2_model3_fasttext.load_state_dict(torch.load(\"t2_model3_fasttext.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "848c6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_score(model,test_input,test_output,length_tensor,name):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        outputs = model(test_input)\n",
    "        predictions = torch.argmax(outputs, dim=2)\n",
    "    \n",
    "        y_pred_padd = [row[:index] for row, index in zip(predictions, length_tensor)] \n",
    "        y_pred_flat = torch.cat(y_pred_padd)\n",
    "        y_padd_tensor =  [row[:index] for row, index in zip(test_output, length_tensor)]\n",
    "        y_labels_flat = torch.cat(y_padd_tensor)\n",
    "        f1 = f1_score(y_labels_flat, y_pred_flat, average='macro')\n",
    "        accuracy = accuracy_score(y_labels_flat, y_pred_flat)\n",
    "        print(f\"Name of model:-{name}, \\t f1 score: {f1},\\t accuracy score: {accuracy} \\n\" )\n",
    "#     return f1,accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef70252d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 NER\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset 1 NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f702d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of model:-t1_model1_word2vec, \t f1 score: 0.3558189079041109,\t accuracy score: 0.861878287002254 \n",
      "\n",
      "Name of model:-t1_model1_glove, \t f1 score: 0.23224886162702643,\t accuracy score: 0.8491960931630353 \n",
      "\n",
      "Name of model:-t1_model1_fasttext, \t f1 score: 0.4028700936231574,\t accuracy score: 0.8789181066867018 \n",
      "\n",
      "Name of model:-t1_model2_word2vec, \t f1 score: 0.3547655557172286,\t accuracy score: 0.8634410217881292 \n",
      "\n",
      "Name of model:-t1_model2_glove, \t f1 score: 0.2362976720430671,\t accuracy score: 0.8332381667918858 \n",
      "\n",
      "Name of model:-t1_model2_fasttext, \t f1 score: 0.3948665838914022,\t accuracy score: 0.8756423741547709 \n",
      "\n",
      "Name of model:-t1_model3_word2vec, \t f1 score: 0.32748062060098565,\t accuracy score: 0.8446882043576258 \n",
      "\n",
      "Name of model:-t1_model3_glove, \t f1 score: 0.21406890878378082,\t accuracy score: 0.8307738542449287 \n",
      "\n",
      "Name of model:-t1_model3_fasttext, \t f1 score: 0.3819970956544048,\t accuracy score: 0.8627798647633358 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_score(model_t1_model1_word2vec,x_test_NER_word2vec_tensor,y_test_NER_tensor,length_test_NER_tensor,\"t1_model1_word2vec\")\n",
    "eval_score(model_t1_model1_glove,x_test_NER_glove_tensor,y_test_NER_tensor,length_test_NER_tensor,\"t1_model1_glove\")\n",
    "eval_score(model_t1_model1_fasttext,x_test_NER_fasttext_tensor,y_test_NER_tensor,length_test_NER_tensor,\"t1_model1_fasttext\")\n",
    "\n",
    "eval_score(model_t1_model2_word2vec,x_test_NER_word2vec_tensor,y_test_NER_tensor,length_test_NER_tensor,\"t1_model2_word2vec\")\n",
    "eval_score(model_t1_model2_glove,x_test_NER_glove_tensor,y_test_NER_tensor,length_test_NER_tensor,\"t1_model2_glove\")\n",
    "eval_score(model_t1_model2_fasttext,x_test_NER_fasttext_tensor,y_test_NER_tensor,length_test_NER_tensor,\"t1_model2_fasttext\")\n",
    "\n",
    "eval_score(model_t1_model3_word2vec,x_test_NER_word2vec_tensor,y_test_NER_tensor,length_test_NER_tensor,\"t1_model3_word2vec\")\n",
    "eval_score(model_t1_model3_glove,x_test_NER_glove_tensor,y_test_NER_tensor,length_test_NER_tensor,\"t1_model3_glove\")\n",
    "eval_score(model_t1_model3_fasttext,x_test_NER_fasttext_tensor,y_test_NER_tensor,length_test_NER_tensor,\"t1_model3_fasttext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c62e6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 2 ATE\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset 2 ATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d3df1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of model:-t2_model1_word2vec, \t f1 score: 0.6999402532919099,\t accuracy score: 0.9062076967704505 \n",
      "\n",
      "Name of model:-t2_model1_glove, \t f1 score: 0.6017197129354663,\t accuracy score: 0.8849352156256043 \n",
      "\n",
      "Name of model:-t2_model1_fasttext, \t f1 score: 0.688587030489776,\t accuracy score: 0.9098820344227422 \n",
      "\n",
      "Name of model:-t2_model2_word2vec, \t f1 score: 0.709729054763831,\t accuracy score: 0.914910075420615 \n",
      "\n",
      "Name of model:-t2_model2_glove, \t f1 score: 0.6708463379432797,\t accuracy score: 0.9021465867337072 \n",
      "\n",
      "Name of model:-t2_model2_fasttext, \t f1 score: 0.7154978149353443,\t accuracy score: 0.9143299168439374 \n",
      "\n",
      "Name of model:-t2_model3_word2vec, \t f1 score: 0.7199227729549523,\t accuracy score: 0.9135563720750338 \n",
      "\n",
      "Name of model:-t2_model3_glove, \t f1 score: 0.6610621281734312,\t accuracy score: 0.9038870624637401 \n",
      "\n",
      "Name of model:-t2_model3_fasttext, \t f1 score: 0.7335586703984592,\t accuracy score: 0.9191645716495842 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_score(model_t2_model1_word2vec,x_test_ATE_word2vec_tensor,y_test_ATE_tensor,length_test_ATE_tensor,\"t2_model1_word2vec\")\n",
    "eval_score(model_t2_model1_glove,x_test_ATE_glove_tensor,y_test_ATE_tensor,length_test_ATE_tensor,\"t2_model1_glove\")\n",
    "eval_score(model_t2_model1_fasttext,x_test_ATE_fasttext_tensor,y_test_ATE_tensor,length_test_ATE_tensor,\"t2_model1_fasttext\")\n",
    "\n",
    "eval_score(model_t2_model2_word2vec,x_test_ATE_word2vec_tensor,y_test_ATE_tensor,length_test_ATE_tensor,\"t2_model2_word2vec\")\n",
    "eval_score(model_t2_model2_glove,x_test_ATE_glove_tensor,y_test_ATE_tensor,length_test_ATE_tensor,\"t2_model2_glove\")\n",
    "eval_score(model_t2_model2_fasttext,x_test_ATE_fasttext_tensor,y_test_ATE_tensor,length_test_ATE_tensor,\"t2_model2_fasttext\")\n",
    "\n",
    "eval_score(model_t2_model3_word2vec,x_test_ATE_word2vec_tensor,y_test_ATE_tensor,length_test_ATE_tensor,\"t2_model3_word2vec\")\n",
    "eval_score(model_t2_model3_glove,x_test_ATE_glove_tensor,y_test_ATE_tensor,length_test_ATE_tensor,\"t2_model3_glove\")\n",
    "eval_score(model_t2_model3_fasttext,x_test_ATE_fasttext_tensor,y_test_ATE_tensor,length_test_ATE_tensor,\"t2_model3_fasttext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9dc985-77d2-4867-b784-fe935ac8167e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
